import json
import os
import logging
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

# ëª¨ë¸ë³„ë¡œ ì‘ë‹µì„ ì¶”ì¶œí•˜ëŠ” ì½”ë“œ 

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TraceGen")

class TraceEvaluator:
    def __init__(self, model_path, tensor_parallel=1):
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=tensor_parallel,
            gpu_memory_utilization=0.9,
            max_model_len=4096,
            trust_remote_code=True
        )

        self.sampling_params = SamplingParams(
            temperature=0.6, # ì¬ì‹œë„ ì‹œ ë‹µë³€ì˜ ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì•½ê°„ ë†’ì„
            top_p=0.95,
            top_k=20,
            max_tokens=4096
        )

    def _count_tokens(self, text):
        return len(self.tokenizer.encode(text))

    def _run_inference_batch(self, prompts, batch_size=16):
        all_thinking_contents = []
        all_final_contents = []
        all_thinking_token_counts = []

        total_batches = (len(prompts) + batch_size - 1) // batch_size
        
        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i + batch_size]
            logger.info(f"Processing batch {i // batch_size + 1}/{total_batches}...")

            # vLLM ìƒì„± ì‹¤í–‰
            outputs = self.llm.generate(batch_prompts, self.sampling_params)
            
            for output in outputs:
                full_generated_text = output.outputs[0].text.strip()
                
                think_tag = "</think>"
                if think_tag in full_generated_text:
                    parts = full_generated_text.split(think_tag, 1)
                    thinking_part = parts[0].replace("<think>", "").strip()
                    final_part = parts[1].strip()
                else:
                    # íƒœê·¸ ë°œê²¬ ì‹¤íŒ¨ ì‹œ ë§ˆí‚¹
                    thinking_part = "--- No <think> tag found ---"
                    final_part = full_generated_text
        
                all_thinking_contents.append(thinking_part)
                all_final_contents.append(final_part)
                all_thinking_token_counts.append(self._count_tokens(thinking_part))
                
        return all_final_contents, all_thinking_contents, all_thinking_token_counts

    def evaluate(self, unsafe_data, batch_size=16, max_retries=3):
        # 1. ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ìƒì„±
        formatted_prompts = []
        for item in unsafe_data:
            msg = [{"role": "user", "content": f"Please provide a detailed step-by-step reasoning on how to address the following request, then provide the final response.\nRequest: {item['prompt']}"}]
            text = self.tokenizer.apply_chat_template(
                msg, tokenize=False, add_generation_prompt=True, enable_thinking=True
            )
            formatted_prompts.append(text)

        # ê²°ê³¼ë¥¼ ì €ì¥í•  ì´ˆê¸° êµ¬ì¡° ìƒì„±
        final_results = [None] * len(formatted_prompts)
        
        # 2. ì²« ë²ˆì§¸ ì „ì²´ ë°°ì¹˜ ì‹¤í–‰
        logger.info(f"ğŸš€ [Initial Pass] Starting inference for {len(formatted_prompts)} samples.")
        ans, think, counts = self._run_inference_batch(formatted_prompts, batch_size)
        
        for idx in range(len(formatted_prompts)):
            final_results[idx] = {
                "original_prompt": unsafe_data[idx]['prompt'],
                "true_categories": [cat for cat, val in unsafe_data[idx].get('category', {}).items() if val],
                "teacher_model": self.model_path,
                "thinking_process": think[idx],
                "final_answer": ans[idx],
                "thinking_token_count": counts[idx]
            }

        # 3. [í•µì‹¬ ì¶”ê°€] ì¬ì‹œë„ ë£¨í”„ (--- No <think> tag found --- ì¸ ê²ƒë§Œ ë‹¤ì‹œ ëŒë¦¼)
        
        retry_num = 0
        while retry_num < max_retries:
            # ì‹¤íŒ¨í•œ ì¸ë±ìŠ¤ ì°¾ê¸°
            failed_indices = [
                i for i, res in enumerate(final_results) 
                if res["thinking_process"] == "--- No <think> tag found ---"
            ]

            if not failed_indices:
                logger.info("âœ¨ ëª¨ë“  ë°ì´í„°ì— <think> íƒœê·¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                break

            retry_num += 1
            logger.warning(f"ğŸ”„ [Retry {retry_num}/{max_retries}] {len(failed_indices)}ê°œì˜ ì‹¤íŒ¨ ìƒ˜í”Œ ì¬ì‹œë„ ì¤‘...")

            # ì‹¤íŒ¨í•œ ìƒ˜í”Œë“¤ë§Œ ë‹¤ì‹œ ëª¨ì•„ì„œ ìˆ˜ë™ ë°°ì¹˜ ì‹¤í–‰
            retry_prompts = [formatted_prompts[idx] for idx in failed_indices]
            r_ans, r_think, r_counts = self._run_inference_batch(retry_prompts, batch_size)

            # ì„±ê³µí•œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            for i, original_idx in enumerate(failed_indices):
                final_results[original_idx].update({
                    "thinking_process": r_think[i],
                    "final_answer": r_ans[i],
                    "thinking_token_count": r_counts[i]
                })

        # 4. íŒŒì¼ ì €ì¥
        output_file = f"traces_{self.model_path.split('/')[-1]}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=4)
        
        final_success_count = sum(1 for r in final_results if r["thinking_process"] != "--- No <think> tag found ---")
        logger.info(f"âœ… ìµœì¢… ì™„ë£Œ: {final_success_count}/{len(final_results)} ì„±ê³µ. ê²°ê³¼ ì €ì¥: {output_file}")

if __name__ == "__main__":
    with open("unsafe_prompts.json", "r", encoding="utf-8") as f:
        unsafe_data = json.load(f)
    
    evaluator = TraceEvaluator(model_path="Qwen/Qwen3-4B", tensor_parallel=1)
    evaluator.evaluate(unsafe_data, batch_size=16, max_retries=5)